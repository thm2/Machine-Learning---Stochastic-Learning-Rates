{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "morPALt2FauX"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOuWh5NNIhWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049a5844-28ac-47eb-a2f4-dba87fd0eb3b"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as lns\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "from functools import reduce\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe14e957510>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL2VXKqwIk0L"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "dnload = True\n",
        "batch_sz = 128\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=dnload, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_sz,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=dnload, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_sz,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUYY4i8NImxa"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = torch.flatten(x, 1)\n",
        "      x = self.fc1(x)\n",
        "\n",
        "      output = x\n",
        "      return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOiyiSdUAk9V"
      },
      "source": [
        "for randseed in range(20):\n",
        "      torch.manual_seed(10+randseed)\n",
        "\n",
        "      net = Net()\n",
        "      net.to(device)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      learning_rate = 0.3\n",
        "      optimizer = optim.SGD(net.parameters(),  lr=0.3)\n",
        "\n",
        "\n",
        "      train_losses = []\n",
        "      val_losses = []\n",
        "      num_epochs = 1500\n",
        "      test_frequency= 1999\n",
        "      batch_no=0\n",
        "      net.train()\n",
        "\n",
        "      for epoch in range(20):  # loop over the dataset multiple times\n",
        "          start_time = time.time()\n",
        "          running_loss = 0.0\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "              net.train()\n",
        "\n",
        "              batch_no+=1\n",
        "\n",
        "              # Update learning rate late in training\n",
        "              if True:\n",
        "                  learning_rate_disc = learning_rate/(np.sqrt(batch_no)/50)\n",
        "              for param_group in optimizer.param_groups:\n",
        "                  param_group['lr'] = learning_rate_disc\n",
        "\n",
        "              inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "              # zero the parameter gradients\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              c1=0.7\n",
        "              c2=1.3\n",
        "\n",
        "              if batch_no >=100:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                  param_group['lr'] =torch.FloatTensor(1).uniform_(c1**(1/(batch_no%100+1)), c2**(1/(batch_no%100+1))).item() * learning_rate\n",
        "    \n",
        "\n",
        "              # forward + backward + optimize\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              \n",
        "\n",
        "              # print statistics\n",
        "              running_loss += loss.item()\n",
        "              if i % 200 ==  199:    # print every 200 mini-batches\n",
        "                  print('[%d, %5d] loss: %.3f' %\n",
        "                        (epoch + 1, i + 1, running_loss / 200))\n",
        "                  train_losses.append(running_loss / 200)\n",
        "                  running_loss = 0.0\n",
        "\n",
        "                  losses = []\n",
        "                  net.eval()\n",
        "                  with torch.no_grad():\n",
        "                      for j, data in enumerate(testloader):\n",
        "                          startt_time = time.time()\n",
        "                          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                          outputs = net(inputs)\n",
        "                          loss = criterion(outputs, labels)\n",
        "                          losses.append(loss.item())\n",
        "                  print('[%d, %5d] test loss: %.3f' %\n",
        "                        (epoch + 1, i + 1,  np.mean((losses)) ))\n",
        "                  val_losses.append(np.mean(losses))\n",
        "                  net.train()\n",
        "\n",
        "\n",
        "# save the results\n",
        "output = [\"{}{}\".format(\"loss: \", \"%.3f\" % i) for i in train_losses]\n",
        "output_conc = output\n",
        "np.savetxt('MNIST_logreg_train'+str(10+randseed)+'.txt', output_conc, fmt=\"%s\")\n",
        "files.download('MNIST_logreg_train'+str(10+randseed)+'.txt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}