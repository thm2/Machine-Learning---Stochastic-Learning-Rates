{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_irlPZjXGfT"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0AOLAH7J46m"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTEN4XJxexuM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as lns\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from google.colab import files\n",
        "from functools import reduce\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "910f6f9cbb1a49c7a3f321d42d6d5286",
            "b2ecfebc4aae4bb5b34c175aff3ef089",
            "2c6218d9f3a2456abee8fecab070bfbe",
            "20a35faf0b0243dd88e83f671dc1434e",
            "31c51d82db8949af810e4b404bda7dc9",
            "16462c73fb514db1970077e4aadd2c49",
            "e1feac7527984985a2b75b472677c1f5",
            "62d19c3b841847f0a92629ed00f7ccaa",
            "14f3d4457a41457b8c08646c68ea77da",
            "0fc41bd904ab49f0a9a7c29700db3e09",
            "d980e2311b2f45398a7a423af22794d3"
          ]
        },
        "id": "SL2VXKqwIk0L",
        "outputId": "c73efa55-cc59-4fbc-b216-1fa6bed0721b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "910f6f9cbb1a49c7a3f321d42d6d5286"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "dnload = True\n",
        "batch_sz = 128\n",
        "\n",
        "# trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        # download=dnload, transform=transform)\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "#                                         download=dnload, transform=transform)\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=dnload, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_sz,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98VK5zmyT_rR"
      },
      "outputs": [],
      "source": [
        "for randseed in range(20):\n",
        "      torch.manual_seed(10+randseed)\n",
        "\n",
        "      \n",
        "      net = models.resnet18()\n",
        "      net.to(device)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # parameters for ADAM related methods\n",
        "      beta2=0.999\n",
        "      beta1=0.9\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      learning_rate = 0.1\n",
        "      optimizer = optim.SGD(net.parameters(),  lr=learning_rate) # learning_rate = 0.1\n",
        "      # optimizer = optim.SGD(net.parameters(),  lr=learning_rate, momentum=0.9, nesterov=True) # learning_rate = 0.01\n",
        "      # optimizer = optim.SGD(net.parameters(),  lr=learning_rate, momentum=0.9, nesterov=False) # learning_rate = 0.01\n",
        "      # optimizer = optim.AdamW(net.parameters(),betas=(0.9, 0.999), lr=learning_rate,amsgrad=True) # learning_rate = 0.001\n",
        "      # optimizer = optim.Adam(net.parameters(),betas=(beta1, beta2), lr=learning_rate,amsgrad=True) # learning_rate = 0.001\n",
        "      # optimizer = optim.Adam(net.parameters(),betas=(beta1, beta2), lr=learning_rate,amsgrad=False) # learning_rate = 0.001\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      train_losses = []\n",
        "      num_epochs = 20\n",
        "      batch_no=0\n",
        "      net.train()\n",
        "      \n",
        "      c1=0.9\n",
        "      c2=1.1\n",
        "\n",
        "      for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "          start_time = time.time()\n",
        "          running_loss = 0.0\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "              net.train()\n",
        "\n",
        "              batch_no+=1\n",
        "\n",
        "              # get the inputs; data is a list of [inputs, labels]\n",
        "              inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "              # zero the parameter gradients\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "\n",
        "              if batch_no >=200:\n",
        "                 for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *=torch.FloatTensor(1).uniform_(c1**(1/(batch_no%100+1)), c2**(1/(batch_no%100+1))).item()\n",
        "                    # param_group['lr'] *=torch.FloatTensor(1).uniform_(c1**(1/(batch_no)), c2**(1/(batch_no))).item()\n",
        "\n",
        "              # forward + backward + optimize\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              \n",
        "              # print statistics\n",
        "              running_loss += loss.item()\n",
        "              if i % 195 ==  194:    # print every 2000 mini-batch\n",
        "                  print('[%d, %5d] loss: %.3f' %\n",
        "                        (epoch + 1, i + 1, running_loss / 194))\n",
        "                  # print(time.time()-start_time)\n",
        "                  train_losses.append(running_loss / 194)\n",
        "                  running_loss = 0.0\n",
        "\n",
        "     # output the results\n",
        "      output = [\"{}{}\".format(\"loss: \", \"%.3f\" % i) for i in train_losses]\n",
        "      output_conc = output\n",
        "      np.savetxt('SGD_train'+str(10+randseed)+'.txt', output_conc, fmt=\"%s\")\n",
        "      files.download('SGD_train'+str(10+randseed)+'.txt')"
      ]
    }
  ]
}